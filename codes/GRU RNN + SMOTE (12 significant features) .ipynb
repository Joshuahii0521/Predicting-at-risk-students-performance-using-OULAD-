{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30a7ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, ensemble, tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV,cross_validate\n",
    "from sklearn.metrics import confusion_matrix, auc,roc_auc_score,roc_curve,recall_score,classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 100, 'display.max_columns', 400)\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, auc,roc_auc_score,roc_curve,recall_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import theano\n",
    "import theano.tensor as Tensor\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import utils\n",
    "import visualize\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, BatchNormalization, Bidirectional, LayerNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "import re\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation, Dense, Flatten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbe3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed,RNN # for creating layers inside the Neural Network\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad4ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = pd.read_csv('data5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38df1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test= train_test_split(data5, test_size=0.3,random_state=123, stratify=data5.Result)# stratify the outcome\n",
    "X_train =train.drop(['Result'], axis = 1)\n",
    "X_test = test.drop(['Result'], axis = 1)\n",
    "Y_train = train['Result'].values\n",
    "Y_test = test['Result'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce16b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 144992\n",
      "Before OverSampling, counts of label '0': 589010 \n",
      "\n",
      "After OverSampling, the shape of train_X: (1178020, 52)\n",
      "After OverSampling, the shape of train_y: (1178020,) \n",
      "\n",
      "After OverSampling, counts of label '1': 589010\n",
      "After OverSampling, counts of label '0': 589010\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(Y_train == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(Y_train == 0)))\n",
    "  \n",
    "# import SMOTE module from imblearn library\n",
    "# pip install imblearn (if you don't have imblearn in your system)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 2)\n",
    "X_train_res, Y_train_res = sm.fit_resample(X_train, Y_train.ravel())\n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(Y_train_res.shape))\n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(Y_train_res == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(Y_train_res == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da83c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res= np.expand_dims(X_train_res, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72054170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178020, 52, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c741d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fe22aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314573, 52, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db7941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras import models \n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    #Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    #If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    #How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131a45ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 52, 128)           50304     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 52, 128)           0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 52, 128)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 52, 128)           99072     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 52, 128)           0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 52, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 187,969\n",
      "Trainable params: 187,329\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# GRU LAYER IS ADDED TO MODEL WITH 128 CELLS IN IT\n",
    "model.add(GRU(128, input_shape=(X_train_res.shape[1],X_train_res.shape[2]), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))  # 20% DROPOUT ADDED FOR REGULARIZATION\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(128, input_shape=(X_train_res.shape[1],X_train_res.shape[2]), activation='tanh', return_sequences=True))   # ADD ANOTHER LAYER\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(64, input_shape=(X_train_res.shape[1],X_train_res.shape[2]), activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))  # FINAL CLASSIFICATION LAYER WITH 2 CLASSES AND SOFTMAX\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# OPTIMIZER SETTINGS\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0017, decay=1e-6)\n",
    "\n",
    "# MODEL COMPILE\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy','Recall','Precision',f1_score])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac36d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2301/2301 [==============================] - 935s 404ms/step - loss: 0.5574 - accuracy: 0.6986 - recall: 0.7198 - precision: 0.6906 - f1_score: 0.7041 - val_loss: 0.4228 - val_accuracy: 0.7824 - val_recall: 0.7699 - val_precision: 0.4691 - val_f1_score: 0.5820\n",
      "Epoch 2/10\n",
      "2301/2301 [==============================] - 936s 407ms/step - loss: 0.3298 - accuracy: 0.8455 - recall: 0.8721 - precision: 0.8280 - f1_score: 0.8494 - val_loss: 0.2620 - val_accuracy: 0.8678 - val_recall: 0.9070 - val_precision: 0.6114 - val_f1_score: 0.7296\n",
      "Epoch 3/10\n",
      "2301/2301 [==============================] - 1015s 441ms/step - loss: 0.2302 - accuracy: 0.8957 - recall: 0.9212 - precision: 0.8765 - f1_score: 0.8982 - val_loss: 0.2049 - val_accuracy: 0.8967 - val_recall: 0.9153 - val_precision: 0.6763 - val_f1_score: 0.7771\n",
      "Epoch 4/10\n",
      "2301/2301 [==============================] - 946s 411ms/step - loss: 0.2056 - accuracy: 0.9064 - recall: 0.9309 - precision: 0.8874 - f1_score: 0.9085 - val_loss: 0.2172 - val_accuracy: 0.8915 - val_recall: 0.9385 - val_precision: 0.6579 - val_f1_score: 0.7728\n",
      "Epoch 5/10\n",
      "2301/2301 [==============================] - 912s 397ms/step - loss: 0.1968 - accuracy: 0.9101 - recall: 0.9346 - precision: 0.8910 - f1_score: 0.9122 - val_loss: 0.2028 - val_accuracy: 0.8933 - val_recall: 0.9391 - val_precision: 0.6621 - val_f1_score: 0.7759\n",
      "Epoch 6/10\n",
      "2301/2301 [==============================] - 879s 382ms/step - loss: 0.1999 - accuracy: 0.9090 - recall: 0.9326 - precision: 0.8905 - f1_score: 0.9110 - val_loss: 0.2104 - val_accuracy: 0.8948 - val_recall: 0.9371 - val_precision: 0.6662 - val_f1_score: 0.7780\n",
      "Epoch 7/10\n",
      "2301/2301 [==============================] - 883s 384ms/step - loss: 0.1877 - accuracy: 0.9138 - recall: 0.9381 - precision: 0.8946 - f1_score: 0.9158 - val_loss: 0.1942 - val_accuracy: 0.8982 - val_recall: 0.9414 - val_precision: 0.6732 - val_f1_score: 0.7844\n",
      "Epoch 8/10\n",
      "2301/2301 [==============================] - 930s 404ms/step - loss: 0.1896 - accuracy: 0.9135 - recall: 0.9374 - precision: 0.8947 - f1_score: 0.9154 - val_loss: 0.1880 - val_accuracy: 0.9028 - val_recall: 0.9345 - val_precision: 0.6867 - val_f1_score: 0.7909\n",
      "Epoch 9/10\n",
      "2301/2301 [==============================] - 887s 385ms/step - loss: 0.1801 - accuracy: 0.9171 - recall: 0.9418 - precision: 0.8975 - f1_score: 0.9191 - val_loss: 0.1803 - val_accuracy: 0.9010 - val_recall: 0.9421 - val_precision: 0.6800 - val_f1_score: 0.7892\n",
      "Epoch 10/10\n",
      "2301/2301 [==============================] - 883s 384ms/step - loss: 0.1838 - accuracy: 0.9156 - recall: 0.9405 - precision: 0.8958 - f1_score: 0.9175 - val_loss: 0.1971 - val_accuracy: 0.8985 - val_recall: 0.9344 - val_precision: 0.6758 - val_f1_score: 0.7835\n"
     ]
    }
   ],
   "source": [
    "# RUN THE MODEL\n",
    "history = model.fit(X_train_res, Y_train_res, epochs=10, batch_size=512, verbose=1,\n",
    "                    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6722d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288dac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4972de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
